{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stretch and Squeeze Psychophysics Data Analysis and Figure Generation\n",
    "\n",
    "This notebook generates figure panels and performs statistical analyses for the psychophysics experiments in the Stretch and Squeeze paper. \n",
    "\n",
    "A de-identified version of all experimental data is being released for this project. The variable DEIDENTIFIED_DATA is \"True\" when using the de-identified dataset for the analysis. Setting it to \"False\" allows authors who have access to the raw source data to generate demographic tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEIDENTIFIED_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Setting fonttype to the meaning of life makes text editable in exported pdfs\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "# Function to calculate bootstrap confidence intervals\n",
    "def bootstrap_ci(data, num_bootstrap_samples=10000, confidence_level=0.95):\n",
    "    bootstrap_means = np.array([np.mean(np.random.choice(data, size=len(data), replace=True)) \n",
    "                                for _ in range(num_bootstrap_samples)])\n",
    "    return np.percentile(bootstrap_means, [(1 - confidence_level) / 2 * 100, (1 + confidence_level) / 2 * 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current working directory to the parent directory (which contains the \"notebooks\" directory among others)\n",
    "changed_dir = False\n",
    "if not changed_dir and os.path.exists(\"./make_figs.ipynb\"):\n",
    "  os.chdir(os.path.dirname(os.getcwd()))\n",
    "  changed_dir = True\n",
    "assert os.path.exists(\"./notebooks/make_figs.ipynb\"), \"Make sure your working directory starts in 'notebooks'\"\n",
    "\n",
    "os.makedirs(\"notebooks/fig_outputs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\"stimulus_image_url_l\", \"stimulus_image_url_r\", \"class_l\", \"class_r\", \"mask_duration_msec\", \"mask_image_url\", \"choice_slot\", \"choice_image_urls\", \"keep_stimulus_on\", \"query_string\", \"platform\", \"bonus_usd_if_correct\"]\n",
    "if DEIDENTIFIED_DATA:\n",
    "  drop_columns.extend([\"assignment_id\", \"worker_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenet12_dataset(data_path, RUN_TESTS=True, drop_columns=None):\n",
    "  assert os.path.isfile(data_path), \"Data file not found: \" + data_path\n",
    "\n",
    "  # Load dataset and convert to a dataframe with 1 row per trial\n",
    "  ds = xr.open_dataset(data_path)\n",
    "\n",
    "  raw_df = ds.to_dataframe().reset_index()\n",
    "\n",
    "  # Filter rows where choice_slot equals i_choice\n",
    "  df = raw_df[raw_df['choice_slot'] == raw_df['i_choice']].copy()\n",
    "\n",
    "  # Sort dataframe such that trials for each participant appear in order\n",
    "  df = df.sort_values(by=['participant', 'obs'])\n",
    "\n",
    "  # Sort the columns in a logical order\n",
    "  ordered_cols = ['participant', 'condition_idx', 'block', 'obs', 'trial_type', 'class', 'stimulus_image_url', 'stimulus_name', 'choice_name', 'i_correct_choice', 'i_choice', 'perf', 'reaction_time_msec', 'rel_timestamp_response', 'timestamp_start', 'monitor_width_px', 'monitor_height_px', 'stimulus_width_px', 'choice_width_px', 'stimulus_duration_msec', 'post_stimulus_delay_duration_msec', 'pre_choice_lockout_delay_duration_msec']\n",
    "  other_cols = [col for col in df.columns if col not in ordered_cols]\n",
    "  df = df[ordered_cols + other_cols]\n",
    "\n",
    "  # Recover info about whether each image was from train, val, etc.\n",
    "  df[\"split\"] = df.apply(lambda row: row['stimulus_image_url'].split(\".s3.amazonaws.com/\")[1].split(\"/\")[0], axis=1)\n",
    "\n",
    "  if RUN_TESTS:\n",
    "    # Sanity check that will almost certainly fail if stimulus_name and choice_name are calculated incorrectly\n",
    "    for _, row in df.iterrows():\n",
    "      if row[\"i_choice\"] == row[\"i_correct_choice\"]:\n",
    "        assert(row[\"stimulus_name\"] == row[\"choice_name\"]), \"stim=\" + row[\"stimulus_name\"] + \", choice=\" + row[\"choice_name\"]\n",
    "      else:\n",
    "        assert(row[\"stimulus_name\"] != row[\"choice_name\"]), \"stim=\" + row[\"stimulus_name\"] + \", choice=\" + row[\"choice_name\"]\n",
    "\n",
    "  if drop_columns is not None:\n",
    "    df = df.drop(drop_columns, axis=1, errors='ignore')\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEIDENTIFIED_DATA and os.path.isfile(\"psych_data/df_main_6ani_6nonani_v1_2.csv\") and os.path.isfile(\"psych_data/df_main_6ani_6nonani_v1_4.csv\") and os.path.isfile(\"psych_data/df_main_6ani_6nonani_v1_6.csv\") and os.path.isfile(\"psych_data/df_main_6ani_6nonani_v2_0.csv\") and os.path.isfile(\"psych_data/df_main_6ani_6nonani_v2_1.csv\"):\n",
    "    print(\"Reading datasets from saved .csv\")\n",
    "    df_main_6ani_6nonani_v1_2 = pd.read_csv(\"psych_data/df_main_6ani_6nonani_v1_2.csv\")\n",
    "    df_main_6ani_6nonani_v1_4 = pd.read_csv(\"psych_data/df_main_6ani_6nonani_v1_4.csv\")\n",
    "    df_main_6ani_6nonani_v1_6 = pd.read_csv(\"psych_data/df_main_6ani_6nonani_v1_6.csv\")\n",
    "    df_main_6ani_6nonani_v2_0 = pd.read_csv(\"psych_data/df_main_6ani_6nonani_v2_0.csv\")\n",
    "    df_main_6ani_6nonani_v2_1 = pd.read_csv(\"psych_data/df_main_6ani_6nonani_v2_1.csv\")\n",
    "else: # Load from raw .h5\n",
    "    print(\"Reading datasets from .h5 files\")\n",
    "\n",
    "    df_main_6ani_6nonani_v1_2 = load_imagenet12_dataset(\"./psych_data/6ani_6nonani_v1_2_combined_dataset.h5\", drop_columns=drop_columns)\n",
    "    df_main_6ani_6nonani_v1_2[\"experiment_id\"] = \"6ani_6nonani_v1_2\"\n",
    "\n",
    "    df_main_6ani_6nonani_v1_4 = load_imagenet12_dataset(\"./psych_data/6ani_6nonani_v1_4_combined_dataset.h5\", drop_columns=drop_columns)\n",
    "    df_main_6ani_6nonani_v1_4[\"experiment_id\"] = \"6ani_6nonani_v1_4\"\n",
    "\n",
    "    df_main_6ani_6nonani_v1_6 = load_imagenet12_dataset(\"./psych_data/6ani_6nonani_v1_6_combined_dataset.h5\", drop_columns=drop_columns)\n",
    "    df_main_6ani_6nonani_v1_6[\"experiment_id\"] = \"6ani_6nonani_v1_6\"\n",
    "\n",
    "    df_main_6ani_6nonani_v2_0 = load_imagenet12_dataset(\"./psych_data/6ani_6nonani_v2_0_combined_dataset.h5\", drop_columns=drop_columns)\n",
    "    df_main_6ani_6nonani_v2_0[\"experiment_id\"] = \"6ani_6nonani_v2_0\"\n",
    "\n",
    "    df_main_6ani_6nonani_v2_1 = load_imagenet12_dataset(\"./psych_data/6ani_6nonani_v2_1_combined_dataset.h5\", drop_columns=drop_columns)\n",
    "    df_main_6ani_6nonani_v2_1[\"experiment_id\"] = \"6ani_6nonani_v2_1\"\n",
    "\n",
    "    if DEIDENTIFIED_DATA:\n",
    "        print(\"Saving de-identified version of the dataset\")\n",
    "        df_main_6ani_6nonani_v1_2.to_csv(\"psych_data/df_main_6ani_6nonani_v1_2.csv\", index=False)\n",
    "        df_main_6ani_6nonani_v1_4.to_csv(\"psych_data/df_main_6ani_6nonani_v1_4.csv\", index=False)\n",
    "        df_main_6ani_6nonani_v1_6.to_csv(\"psych_data/df_main_6ani_6nonani_v1_6.csv\", index=False)\n",
    "        df_main_6ani_6nonani_v2_0.to_csv(\"psych_data/df_main_6ani_6nonani_v2_0.csv\", index=False)\n",
    "        df_main_6ani_6nonani_v2_1.to_csv(\"psych_data/df_main_6ani_6nonani_v2_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which version of the dataset to use\n",
    "\n",
    "df_main_6ani_6nonani = df_main_6ani_6nonani_v2_1  # v2_1 is the version with 25 participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Remove participants with less than 500 trials (e.g., those who didn't pass the screening phase)\n",
    "\n",
    "df_main_6ani_6nonani = df_main_6ani_6nonani[df_main_6ani_6nonani['block'] > 0]\n",
    "unique_obs_counts_per_row = df_main_6ani_6nonani.groupby('participant')['obs'].transform('nunique')\n",
    "df_main_6ani_6nonani = df_main_6ani_6nonani[unique_obs_counts_per_row >= 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Reconstruct split names\n",
    "\n",
    "def reconstruct_split_name(url):\n",
    "    \"\"\"\n",
    "    Extracts parts from the stimulus_image_url to create a new split name.\n",
    "    Expected format: \"base_folder-filename_part\"\n",
    "    Example Input: \"https://.../vanilla resnet50/unit [805]/Stretch in conv25/unit_[805]_Stretch in conv25_1.png\"\n",
    "    Example Output: \"vanilla_resnet50-Stretch_in_conv25\"\n",
    "    \"\"\"\n",
    "    if pd.isna(url) or not isinstance(url, str):\n",
    "        return None # Return None for missing or non-string URLs\n",
    "\n",
    "    try:\n",
    "        # Split the URL by '/'\n",
    "        parts = url.split('/')\n",
    "\n",
    "        # --- Part 1: Base-level folder name ---\n",
    "        # Assuming it's the first folder after the domain name (e.g., index 3)\n",
    "        if len(parts) > 3:\n",
    "            base_folder_raw = parts[3]\n",
    "            # Replace spaces with underscores\n",
    "            part1 = base_folder_raw.replace(' ', '_')\n",
    "        else:\n",
    "            # Handle unexpected URL structure\n",
    "            # print(f\"Warning: Could not extract base folder from URL: {url}\")\n",
    "            return None\n",
    "\n",
    "        # --- Part 2: Filename part ---\n",
    "        filename = parts[-1] # Get the last part (filename)\n",
    "\n",
    "        # Use regex to extract the part between unit_[<int>]_ and the final _<int>.png\n",
    "        # Pattern explanation:\n",
    "        # unit_\\[\\d+\\]_  : Matches \"unit_[\", one or more digits, \"]_\" literally\n",
    "        # (.*?)          : Captures any characters non-greedily (this is our target)\n",
    "        # _\\d+\\.png$     : Matches \"_\", one or more digits, \".png\" at the end of the string\n",
    "        match = re.search(r'unit_\\[\\d+\\]_(.*?)_\\d+\\.png$', filename)\n",
    "\n",
    "        if match:\n",
    "            filename_part_raw = match.group(1) # Get the captured group\n",
    "            # Replace spaces with underscores\n",
    "            part2 = filename_part_raw.replace(' ', '_')\n",
    "        else:\n",
    "            # Handle cases where the filename doesn't match the expected pattern\n",
    "            # print(f\"Warning: Could not extract filename part from: {filename} in URL: {url}\")\n",
    "            return None # Or potentially try alternative extraction logic if needed\n",
    "\n",
    "        # --- Combine the parts ---\n",
    "        return f\"{part1}-{part2}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors during processing\n",
    "        print(f\"Error processing URL: {url} - Error: {e}\")\n",
    "        return None\n",
    "\n",
    "condition_not_natural = df_main_6ani_6nonani['split'] != 'natural'\n",
    "\n",
    "#  This creates a Series containing the reconstructed names, indexed like the original non-natural rows\n",
    "reconstructed_names = df_main_6ani_6nonani.loc[condition_not_natural, 'stimulus_image_url'].apply(reconstruct_split_name)\n",
    "\n",
    "# Initialize the 'split_recon' column with 'natural' as the default for ALL rows\n",
    "df_main_6ani_6nonani['split_recon'] = 'natural'\n",
    "\n",
    "# Use .loc with the condition again to overwrite the 'split_recon' values\n",
    "#    ONLY for the non-natural rows, using the 'reconstructed_names' Series.\n",
    "df_main_6ani_6nonani.loc[condition_not_natural, 'split_recon'] = reconstructed_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Handle NaNs in 'perf' column\n",
    "\n",
    "# Method 1: Replace NaNs with 0\n",
    "# print(\"Original 'perf' value counts (including NaN):\")\n",
    "# print(df_main_6ani_6nonani['perf'].value_counts(dropna=False))\n",
    "# df_main_6ani_6nonani['perf_cleaned'] = df_main_6ani_6nonani['perf'].fillna(0)\n",
    "# print(\"\\n'perf_cleaned' value counts (NaN replaced with 0):\")\n",
    "# print(df_main_6ani_6nonani['perf_cleaned'].value_counts(dropna=False))\n",
    "\n",
    "# Method 2: Drop rows with NaNs\n",
    "df_main_6ani_6nonani = df_main_6ani_6nonani.dropna(subset=['perf'])\n",
    "df_main_6ani_6nonani[\"perf_cleaned\"] = df_main_6ani_6nonani[\"perf\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "df_main_6ani_6nonani.to_csv(\"psych_data/df_main_6ani_6nonani_v2_1_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (boxplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = \"split_recon\"\n",
    "\n",
    "df_aggregated = df_main_6ani_6nonani.groupby(['participant', split_col])['perf_cleaned'].mean().reset_index()\n",
    "df_aggregated.rename(columns={'perf_cleaned': 'participant_accuracy'}, inplace=True)\n",
    "\n",
    "def map_split_to_plot_group_and_label(split_name):\n",
    "    if split_name == 'natural':\n",
    "        return \"natural_group\", \"natural\"\n",
    "    elif split_name == 'robust_resnet50-mXDREAM_-_l2robust':\n",
    "        return \"Robust_MEI_group\", \"Robust MEI\"\n",
    "    elif split_name == 'robust_resnet50-Stretch_in_pixelspace':\n",
    "        return \"Robust_Pixel_space_group\", \"Pixel space\"\n",
    "    elif split_name == 'robust_resnet50-Stretch_in_conv25':\n",
    "        return \"Robust_Layer3_conv1_group\", \"Layer3_conv1\"\n",
    "    elif split_name == 'robust_resnet50-Stretch_in_conv51':\n",
    "        return \"Robust_Layer4_conv7_group\", \"Layer4_conv7\"\n",
    "    elif split_name == 'vanilla_resnet50-mXDREAM_-_vanilla':\n",
    "        return \"Standard_MEI_group\", \"Standard MEI\"\n",
    "    elif split_name == 'vanilla_resnet50-Stretch_in_pixelspace':\n",
    "        return \"Standard_Pixel_Space_group\", \"Pixel Space\"\n",
    "    elif split_name == 'vanilla_resnet50-Stretch_in_conv25':\n",
    "        return \"Standard_Layer3_conv1_group\", \"Layer3_conv1\"\n",
    "    elif split_name == 'vanilla_resnet50-Stretch_in_conv51':\n",
    "        return \"Standard_Layer4_conv7_group\", \"Layer4_conv7\"\n",
    "    else:\n",
    "        return f\"Unknown_{split_name}\", f\"Unknown: {split_name}\"\n",
    "\n",
    "mapped_values = df_aggregated[split_col].apply(map_split_to_plot_group_and_label)\n",
    "df_aggregated['plot_group_id'] = [item[0] for item in mapped_values]\n",
    "\n",
    "plot_group_id_order = [\n",
    "    \"natural_group\",\n",
    "    \"Robust_MEI_group\", \"Robust_Pixel_space_group\", \"Robust_Layer3_conv1_group\", \"Robust_Layer4_conv7_group\",\n",
    "    \"Standard_MEI_group\", \"Standard_Pixel_Space_group\", \"Standard_Layer3_conv1_group\", \"Standard_Layer4_conv7_group\"\n",
    "]\n",
    "\n",
    "x_tick_display_labels = [\n",
    "    'natural',\n",
    "    'Robust MEI', 'Pixel space', 'Layer3_conv1', 'Layer4_conv7',\n",
    "    'Standard MEI', 'Pixel Space', 'Layer3_conv1', 'Layer4_conv7'\n",
    "]\n",
    "\n",
    "intra_group_spacing = 1.0\n",
    "inter_group_spacing_val = 1.8\n",
    "\n",
    "x_positions = []\n",
    "current_x = 0.0\n",
    "x_positions.append(round(current_x, 5))\n",
    "\n",
    "current_x += inter_group_spacing_val\n",
    "num_robust_items = 4\n",
    "for i in range(num_robust_items):\n",
    "    x_positions.append(round(current_x + i * intra_group_spacing, 5))\n",
    "\n",
    "current_x = round((current_x + (num_robust_items - 1) * intra_group_spacing) + inter_group_spacing_val, 5)\n",
    "num_standard_items = 4\n",
    "for i in range(num_standard_items):\n",
    "    x_positions.append(round(current_x + i * intra_group_spacing, 5))\n",
    "\n",
    "# Colors\n",
    "natural_color = 'dodgerblue'\n",
    "robust_mei_color = '#92edd1'\n",
    "robust_general_color = '#1b9e77'\n",
    "standard_mei_color = '#ffcdd2'\n",
    "standard_color = '#e57373'\n",
    "\n",
    "color_palette_categorical = {\n",
    "    \"natural_group\": natural_color,\n",
    "    \"Robust_MEI_group\": robust_mei_color,\n",
    "    \"Robust_Pixel_space_group\": robust_general_color,\n",
    "    \"Robust_Layer3_conv1_group\": robust_general_color,\n",
    "    \"Robust_Layer4_conv7_group\": robust_general_color,\n",
    "    \"Standard_MEI_group\": standard_mei_color,\n",
    "    \"Standard_Pixel_Space_group\": standard_color,\n",
    "    \"Standard_Layer3_conv1_group\": standard_color,\n",
    "    \"Standard_Layer4_conv7_group\": standard_color,\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('seaborn-v0_8-ticks') \n",
    "fig, ax = plt.subplots(figsize=(17, 10))\n",
    "\n",
    "fontsize_axis_labels = 28\n",
    "fontsize_tick_labels = 22\n",
    "fontsize_legend = 28\n",
    "linewidth_spines = 3.5\n",
    "linewidth_ticks = 3.5\n",
    "tick_length_major = 15.0\n",
    "linewidth_box = 1.5\n",
    "linewidth_median = 2.0\n",
    "linewidth_whisker = 1.5\n",
    "linewidth_cap = 1.5\n",
    "markeredgewidth_flier = 1.0\n",
    "markersize_flier = 6\n",
    "\n",
    "box_width = 0.7 * intra_group_spacing\n",
    "\n",
    "# Loop through groups and plot each box using ax.boxplot()\n",
    "for i, group_id in enumerate(plot_group_id_order):\n",
    "    group_df = df_aggregated[df_aggregated['plot_group_id'] == group_id]\n",
    "    participant_accuracies = group_df['participant_accuracy'].dropna() # Ensure no NaNs for plt.boxplot\n",
    "\n",
    "    current_x_position = x_positions[i]\n",
    "    current_color = color_palette_categorical[group_id]\n",
    "\n",
    "    bp = ax.boxplot(\n",
    "        participant_accuracies,\n",
    "        positions=[current_x_position], # Sets the x-coordinate for this specific box\n",
    "        widths=box_width,\n",
    "        patch_artist=True,\n",
    "        manage_ticks=False, # will handle all tick configurations manually\n",
    "        boxprops={'edgecolor': 'black', 'linewidth': linewidth_box, 'facecolor': current_color},\n",
    "        medianprops={'color': 'black', 'linewidth': linewidth_median},\n",
    "        whiskerprops={'color': 'black', 'linewidth': linewidth_whisker},\n",
    "        capprops={'color': 'black', 'linewidth': linewidth_cap},\n",
    "        flierprops={'marker': 'o', 'markersize': markersize_flier,\n",
    "                    'markerfacecolor': 'white', 'markeredgecolor': 'black',\n",
    "                    'markeredgewidth': markeredgewidth_flier}\n",
    "    )\n",
    "\n",
    "# Style the plot\n",
    "ax.set_xlabel('Representation space', fontsize=fontsize_axis_labels, labelpad=25)\n",
    "ax.set_ylabel('Accuracy', fontsize=fontsize_axis_labels, labelpad=20)\n",
    "\n",
    "ax.set_xticks(x_positions) # Set tick marks AT the positions of our boxes\n",
    "ax.set_xticklabels(x_tick_display_labels, rotation=45, ha='center', fontsize=fontsize_tick_labels)\n",
    "\n",
    "ax.tick_params(axis='y', labelsize=fontsize_tick_labels)\n",
    "ax.set_ylim(-0.03, 1.0)\n",
    "\n",
    "# Adjust x-axis limits to give some padding around the boxes\n",
    "ax.set_xlim(x_positions[0] - intra_group_spacing * 0.7, x_positions[-1] + intra_group_spacing * 0.7)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(linewidth_spines)\n",
    "ax.spines['bottom'].set_linewidth(linewidth_spines)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', direction='out',\n",
    "               width=linewidth_ticks, length=tick_length_major)\n",
    "\n",
    "# Custom legend\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=robust_general_color, label='Robust'),\n",
    "    mpatches.Patch(color=standard_color, label='Standard')\n",
    "]\n",
    "ax.legend(handles=legend_patches,\n",
    "          loc='lower left',\n",
    "          frameon=False,\n",
    "          fontsize=fontsize_legend,\n",
    "          ncol=1,\n",
    "          handlelength=2.0,\n",
    "          handleheight=0.8,\n",
    "          labelspacing=0.7\n",
    "         )\n",
    "\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"notebooks/fig_outputs/boxplot_accuracy_by_split.pdf\", format='pdf', bbox_inches='tight', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting (means with bootstrap confidence intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Performance per Split\n",
    "mean_perf = df_main_6ani_6nonani.groupby(split_col)['perf_cleaned'].mean()\n",
    "\n",
    "# Calculate 95% Confidence Intervals per Split using bootstrap\n",
    "# Apply the bootstrap function to the 'perf_cleaned' data for each group\n",
    "ci_perf = df_main_6ani_6nonani.groupby(split_col)['perf_cleaned'].apply(\n",
    "    lambda x: bootstrap_ci(x, num_bootstrap_samples=10000, confidence_level=0.95)\n",
    ")\n",
    "\n",
    "# Combine means and CIs into a summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'mean': mean_perf,\n",
    "    'ci': ci_perf\n",
    "})\n",
    "# Split the CI tuple into separate lower and upper bound columns\n",
    "summary_df[['ci_lower', 'ci_upper']] = pd.DataFrame(summary_df['ci'].tolist(), index=summary_df.index)\n",
    "summary_df = summary_df.drop(columns='ci') # Remove the original tuple column\n",
    "\n",
    "# Sort the DataFrame according to requirements\n",
    "# Get unique split names\n",
    "split_names = summary_df.index.unique().tolist()\n",
    "\n",
    "# Sort alphabetically, but ensure 'natural' is first if it exists\n",
    "if 'natural' in split_names:\n",
    "    split_names.remove('natural')\n",
    "    sorted_splits = ['natural'] + sorted(split_names)\n",
    "else:\n",
    "    sorted_splits = sorted(split_names)\n",
    "\n",
    "# Reindex the summary DataFrame based on the desired order\n",
    "summary_df = summary_df.loc[sorted_splits]\n",
    "\n",
    "# Reset index to make 'split' a regular column for plotting\n",
    "summary_df = summary_df.reset_index()\n",
    "\n",
    "print(\"\\nSummary Statistics (Sorted):\")\n",
    "print(summary_df)\n",
    "\n",
    "# Create the Bar Plot with Confidence Intervals\n",
    "\n",
    "# Calculate the error values for the plot (distance from the mean to the CI bounds)\n",
    "# yerr should be in the format [[lower_errors], [upper_errors]]\n",
    "lower_error = summary_df['mean'] - summary_df['ci_lower']\n",
    "upper_error = summary_df['ci_upper'] - summary_df['mean']\n",
    "error_bars = [lower_error, upper_error]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6)) # Adjust figure size as needed\n",
    "bars = plt.bar(summary_df[split_col], summary_df['mean'], yerr=error_bars, capsize=5, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Split Condition', fontsize=12)\n",
    "plt.ylabel('Mean Performance (Accuracy)', fontsize=12)\n",
    "plt.title('Mean Performance by Split with 95% Bootstrap CI (N=10,000)', fontsize=14)\n",
    "\n",
    "# Improve readability\n",
    "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels if they overlap\n",
    "plt.ylim(0, 1.05) # Set y-axis limits appropriate for accuracy (0 to 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7) # Add horizontal grid lines\n",
    "\n",
    "# Add mean values on top of bars for clarity\n",
    "for bar, mean_val in zip(bars, summary_df['mean']):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.02, f'{mean_val:.3f}', va='bottom', ha='center', fontsize=9)\n",
    "\n",
    "# Ensure tight layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical analysis in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Rscript psych_code/scripts/glmm.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Rscript psych_code/scripts/alexnet_glmm.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of missingness by participant (trials where the participant did not respond)\n",
    "\n",
    "# Group by 'participant' and then aggregate:\n",
    "# - Count the number of rows in each group (using 'size')\n",
    "# - Find the maximum value of 'obs' in each group (using 'max')\n",
    "summary_stats = df_main_6ani_6nonani.groupby('participant').agg(\n",
    "    number_of_rows=('participant', 'size'),  # 'size' counts rows in each group\n",
    "    max_obs_value=('obs', 'max')             # 'max' finds max of 'obs' column per group\n",
    ")\n",
    "\n",
    "# Print the results for each participant\n",
    "print(\"Summary for each participant:\")\n",
    "for participant_id, data_row in summary_stats.iterrows():\n",
    "    print(f\"Participant ID: {participant_id}\")\n",
    "    print(f\"  Number of rows: {data_row['number_of_rows']}\")\n",
    "    print(f\"  Max value of obs: {data_row['max_obs_value']}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stretch_squeeze_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
