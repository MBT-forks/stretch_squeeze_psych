{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from lwise_imgproc_utils.enhance_helpers import *\n",
    "\n",
    "def ensure_dir_exists(path):\n",
    "    \"\"\"Create directory and all parent directories if they don't exist.\"\"\"\n",
    "    directory = os.path.dirname(path)\n",
    "    if directory:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    \"\"\"Load image and convert to tensor in range [0,1]\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def create_visualization_grid(orig_dir, mod_dir, save_dir, n_images=8, specific_images=None, \n",
    "                            figsize=(20, 10), figure_save_path=None, seed=None, \n",
    "                            smooth_kernel_size=5, alpha=0.7, magnification_factor=5, splits=[\"val\"], convert_to_ext=None):\n",
    "    \"\"\"\n",
    "    Create a grid visualization and save all individual images.\n",
    "    \n",
    "    Args:\n",
    "        orig_dir: Directory containing original images and dirmap.csv\n",
    "        mod_dir: Directory containing modified images\n",
    "        save_dir: Directory to save individual processed images\n",
    "        n_images: Number of images to display (default 8, used only if specific_images not provided)\n",
    "        specific_images: List of specific image paths to use (optional)\n",
    "        figsize: Base figure size in inches (width, height) - will be adjusted based on n_images\n",
    "        figure_save_path: Path to save the visualization figure (optional)\n",
    "        seed: Random seed for reproducibility (optional)\n",
    "        smooth_kernel_size: Kernel size for heatmap smoothing\n",
    "        alpha: Transparency for heatmap overlay\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        \n",
    "    # Read directory mapping\n",
    "    dirmap = pd.read_csv(os.path.join(orig_dir, 'dirmap.csv'))\n",
    "\n",
    "    dirmap = dirmap[dirmap[\"split\"].isin(splits)]\n",
    "    \n",
    "    # Select images\n",
    "    if specific_images is not None:\n",
    "        selected_paths = specific_images\n",
    "        n_images = len(specific_images)  # Adjust n_images based on provided paths\n",
    "    else:\n",
    "        selected_paths = random.sample(dirmap['im_path'].tolist(), n_images)\n",
    "    \n",
    "    # Adjust figure size based on number of images\n",
    "    adjusted_figsize = (figsize[0] * n_images / 8, figsize[1])\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(4, n_images, figsize=adjusted_figsize)\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    \n",
    "    # Handle case where n_images = 1 (axes would be 1D)\n",
    "    if n_images == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Row titles\n",
    "    row_titles = ['Original', 'Enhanced', 'Difference\\n(magnified 5x)', 'Difference\\nheatmap', ]\n",
    "    for idx, title in enumerate(row_titles):\n",
    "        fig.text(0.02, 0.75 - idx*0.25, title, \n",
    "                va='center', ha='right', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for col, im_path in enumerate(selected_paths):\n",
    "        # Load original and modified images\n",
    "        orig_path = os.path.join(orig_dir, im_path)\n",
    "        mod_path = os.path.join(mod_dir, im_path)\n",
    "        \n",
    "        orig_img = load_and_process_image(orig_path)\n",
    "        mod_img = load_and_process_image(mod_path)\n",
    "        \n",
    "        # Generate difference\n",
    "        im_diff = mod_img - orig_img\n",
    "        #im_diff_norm = (im_diff - im_diff.min()) / (im_diff.max() - im_diff.min())\n",
    "\n",
    "        #im_diff_norm = im_diff\n",
    "\n",
    "        # diff_clipped = torch.clamp(im_diff, -1.0, 1.0)\n",
    "        # im_diff_norm = (diff_clipped + 1.0) / 2.0\n",
    "\n",
    "        im_diff_magnified = im_diff*magnification_factor\n",
    "        diff_clipped = torch.clamp(im_diff_magnified, -1.0, 1.0)\n",
    "        im_diff_norm = (diff_clipped + 1.0) / 2.0\n",
    "        \n",
    "        # Generate heatmap and overlay\n",
    "        heatmap = create_heatmap(im_diff, smooth_kernel_size)\n",
    "        overlay = create_overlay(orig_img, heatmap, alpha)\n",
    "        \n",
    "        # Save individual images\n",
    "        save_base_path = os.path.join(save_dir, im_path)\n",
    "        base_path, ext = os.path.splitext(save_base_path)\n",
    "        \n",
    "        # Ensure directory exists for each image\n",
    "        ensure_dir_exists(save_base_path)\n",
    "\n",
    "        if convert_to_ext:\n",
    "            ext = convert_to_ext\n",
    "        \n",
    "        # Save all versions\n",
    "        save_paths = {\n",
    "            'orig': f\"{base_path}_ORIG{ext}\",\n",
    "            'mod': f\"{base_path}_ENH{ext}\",\n",
    "            'overlay': f\"{base_path}_HEATMAP_OVERLAID_ORIG{ext}\",\n",
    "            'diff': f\"{base_path}_DIFF{ext}\",\n",
    "            'heatmap': f\"{base_path}_HEATMAP{ext}\"\n",
    "        }\n",
    "        \n",
    "        # Save images\n",
    "        save_image(orig_img, save_paths['orig'])\n",
    "        save_image(mod_img, save_paths['mod'])\n",
    "        save_image(overlay, save_paths['overlay'])\n",
    "        save_image(im_diff_norm, save_paths['diff'])\n",
    "        save_image(heatmap, save_paths['heatmap'])\n",
    "        \n",
    "        # Collect visualizations for plotting\n",
    "        images = {\n",
    "            'orig': orig_img,\n",
    "            'mod': mod_img,\n",
    "            'diff': im_diff_norm,\n",
    "            'overlay': overlay,\n",
    "        }\n",
    "        \n",
    "        # Plot each image in its respective position\n",
    "        for row, (key, img) in enumerate(images.items()):\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Convert tensor to numpy and handle channel dimension\n",
    "            if img.shape[0] == 1:  # Grayscale\n",
    "                img_np = img.squeeze(0).numpy()\n",
    "                ax.imshow(img_np, cmap='gray')\n",
    "            else:  # RGB\n",
    "                img_np = img.permute(1, 2, 0).numpy()\n",
    "                ax.imshow(np.clip(img_np, 0, 1))\n",
    "            \n",
    "            # Remove axes ticks\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            # Add column title (only for first row)\n",
    "            if row == 0:\n",
    "                ax.set_title(f'Image {col+1}', pad=5)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0.05, 0, 1, 1])\n",
    "    \n",
    "    # Save figure if requested\n",
    "    if figure_save_path:\n",
    "        # Ensure directory exists for figure\n",
    "        ensure_dir_exists(figure_save_path)\n",
    "        # Save as PDF with 300 DPI\n",
    "        plt.savefig(figure_save_path, bbox_inches='tight', dpi=600, format='pdf')\n",
    "        \n",
    "    return fig, axes\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Basic usage with random images:\n",
    "fig, axes = create_visualization_grid(\n",
    "    orig_dir='path/to/original/images',\n",
    "    mod_dir='path/to/modified/images',\n",
    "    save_dir='path/to/save/directory',\n",
    "    figure_save_path='visualization_grid.pdf',\n",
    "    smooth_kernel_size=5,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Usage with specific images:\n",
    "specific_images = [\n",
    "    'path/to/image1.png',\n",
    "    'path/to/image2.png',\n",
    "    'path/to/image3.png'\n",
    "]\n",
    "fig, axes = create_visualization_grid(\n",
    "    orig_dir='path/to/original/images',\n",
    "    mod_dir='path/to/modified/images',\n",
    "    save_dir='path/to/save/directory',\n",
    "    specific_images=specific_images,\n",
    "    figure_save_path='visualization_grid_specific.pdf',\n",
    "    smooth_kernel_size=5,\n",
    "    alpha=0.7\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_images=[\n",
    "    \"images/ISIC_0024315.jpg\",\n",
    "    \"images/ISIC_0024705.jpg\",\n",
    "    \"images/ISIC_0024912.jpg\",\n",
    "    \"images/ISIC_0031298.jpg\",\n",
    "]\n",
    "fig, axes = create_visualization_grid(\n",
    "    orig_dir='../imgproc_code/data/HAM10000/HAM10000_natural',\n",
    "    mod_dir='../imgproc_code/data/HAM10000/HAM10000_8_0.5_16_logit_diverge',\n",
    "    save_dir='heatmap_viz/ham4',\n",
    "    convert_to_ext='.png',\n",
    "    figure_save_path='visualization_grid.png',\n",
    "    specific_images=specific_images,\n",
    "    smooth_kernel_size=21,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_images=[\n",
    "    \"images/MHIST_ayy.png\",\n",
    "    \"images/MHIST_bsy.png\",\n",
    "]\n",
    "fig, axes = create_visualization_grid(\n",
    "    orig_dir='../imgproc_code/data/mhist/mhist_original',\n",
    "    mod_dir='../imgproc_code/data/mhist/mhist_8_0.5_16_logit_diverge',\n",
    "    save_dir='heatmap_viz/mhist',\n",
    "    convert_to_ext='.png',\n",
    "    figure_save_path='visualization_grid.png',\n",
    "    specific_images=specific_images,\n",
    "    smooth_kernel_size=21,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_images=[\n",
    "    \"train/01233_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_aversata/d8e21ccf-cd9c-46f0-826e-d962d61b0a62.jpg\",\n",
    "    \"train/01234_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_biselata/b365cd7a-8905-441e-a0f5-735d146d66ff.jpg\",\n",
    "    \"train/01239_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_seriata/dc9bae6a-9ec0-4fa7-abae-14596d9d601e.jpg\",\n",
    "    \"train/01240_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_tacturata/33dc5c3d-a080-4a3f-8a12-52cdb42c5576.jpg\",\n",
    "]\n",
    "fig, axes = create_visualization_grid(\n",
    "    orig_dir='../imgproc_code/data/idaea4/idaea4_natural',\n",
    "    mod_dir='../imgproc_code/data/idaea4/idaea4_8_0.5_16_logit_diverge',\n",
    "    save_dir='heatmap_viz/idaea4',\n",
    "    convert_to_ext='.png',\n",
    "    figure_save_path='visualization_grid.png',\n",
    "    specific_images=specific_images,\n",
    "    smooth_kernel_size=21,\n",
    "    splits=[\"train\", \"val\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_images=[\n",
    "    \"val/monkey/ILSVRC2012_val_00031957.JPEG\",\n",
    "    \"val/lizard/ILSVRC2012_val_00013870.JPEG\",\n",
    "    \"val/insect/ILSVRC2012_val_00013395.JPEG\",\n",
    "    \"val/fish/ILSVRC2012_val_00047682.JPEG\",\n",
    "    \"val/crab/ILSVRC2012_val_00020816.JPEG\",\n",
    "    \"val/bird/ILSVRC2012_val_00030431.JPEG\",\n",
    "    \"val/dog/ILSVRC2012_val_00030567.JPEG\",\n",
    "]\n",
    "fig, axes = create_visualization_grid(\n",
    "    orig_dir='../imgproc_code/data/imagenet16_resized',\n",
    "    mod_dir='../imgproc_code/data/imagenet16_20_0.5_40_logit',\n",
    "    figure_save_path='fig_outputs/visualization_grid.pdf',\n",
    "    specific_images=specific_images,\n",
    "    save_dir='heatmap_viz/imagenet16',\n",
    "    convert_to_ext='.png',\n",
    "    smooth_kernel_size=21,\n",
    "    splits=[\"val\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_average_heatmap(orig_dir, mod_dir, dirmap_path, save_dir, \n",
    "                           figure_save_path=None, smooth_kernel_size=5, \n",
    "                           splits=[\"val\"], convert_to_ext=None):\n",
    "    \"\"\"\n",
    "    Create an average heatmap across all images in the specified splits.\n",
    "    \n",
    "    Args:\n",
    "        orig_dir: Directory containing original images\n",
    "        mod_dir: Directory containing modified images\n",
    "        dirmap_path: Path to the CSV file containing image paths and splits\n",
    "        save_dir: Directory to save the average heatmap\n",
    "        figure_save_path: Path to save the visualization figure (optional)\n",
    "        smooth_kernel_size: Kernel size for heatmap smoothing\n",
    "        splits: List of splits to include (e.g., [\"train\", \"val\", \"test\"])\n",
    "        convert_to_ext: Extension to convert images to when saving (optional)\n",
    "    \n",
    "    Returns:\n",
    "        avg_heatmap: The average heatmap as a tensor\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    from torchvision.utils import save_image\n",
    "    \n",
    "    # Read directory mapping\n",
    "    dirmap = pd.read_csv(dirmap_path)\n",
    "    \n",
    "    # Filter by splits\n",
    "    dirmap = dirmap[dirmap[\"split\"].isin(splits)]\n",
    "    \n",
    "    print(f\"Processing {len(dirmap)} images from splits: {splits}\")\n",
    "    \n",
    "    # Variables to accumulate difference magnitudes directly\n",
    "    magnitude_sum = None\n",
    "    count = 0\n",
    "    \n",
    "    # Process each image\n",
    "    for idx, row in enumerate(dirmap.itertuples()):\n",
    "        im_path = row.im_path\n",
    "        \n",
    "        # Load original and modified images\n",
    "        orig_path = os.path.join(orig_dir, im_path)\n",
    "        mod_path = os.path.join(mod_dir, im_path)\n",
    "        \n",
    "        try:\n",
    "            orig_img = load_and_process_image(orig_path)\n",
    "            mod_img = load_and_process_image(mod_path)\n",
    "            \n",
    "            # Generate difference\n",
    "            im_diff = mod_img - orig_img\n",
    "            \n",
    "            # Calculate magnitude of difference across channels (similar to create_heatmap)\n",
    "            if im_diff.shape[0] == 3:  # RGB image\n",
    "                magnitude = torch.sqrt(torch.sum(im_diff ** 2, dim=0))\n",
    "            else:\n",
    "                magnitude = torch.abs(im_diff.squeeze())\n",
    "            \n",
    "            # Accumulate magnitude (without normalization)\n",
    "            if magnitude_sum is None:\n",
    "                magnitude_sum = magnitude.clone()\n",
    "                image_shape = magnitude.shape\n",
    "            else:\n",
    "                # Handle potential dimension mismatch by skipping\n",
    "                if magnitude.shape != image_shape:\n",
    "                    print(f\"Warning: Image {im_path} has different dimensions. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                magnitude_sum += magnitude\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            # Print progress every 100 images\n",
    "            if (idx + 1) % 100 == 0 or (idx + 1) == len(dirmap):\n",
    "                print(f\"Processed {idx + 1}/{len(dirmap)} images\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {im_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate average magnitude\n",
    "    if count > 0:\n",
    "        avg_magnitude = magnitude_sum / count\n",
    "        print(f\"Created average magnitude from {count} images\")\n",
    "    else:\n",
    "        raise ValueError(\"No valid images were processed\")\n",
    "    \n",
    "    # Optional smoothing of the averaged magnitude\n",
    "    if smooth_kernel_size:\n",
    "        kernel_size = smooth_kernel_size\n",
    "        channels = 1\n",
    "        kernel = torch.ones(channels, 1, kernel_size, kernel_size) / (kernel_size * kernel_size)\n",
    "        avg_magnitude = avg_magnitude.unsqueeze(0).unsqueeze(0)\n",
    "        avg_magnitude = F.conv2d(avg_magnitude, kernel.to(avg_magnitude.device), padding=kernel_size//2)\n",
    "        avg_magnitude = avg_magnitude.squeeze()\n",
    "    \n",
    "    # Print statistics before normalization\n",
    "    print(f\"Average magnitude statistics - Min: {avg_magnitude.min().item()}, Max: {avg_magnitude.max().item()}, Mean: {avg_magnitude.mean().item()}\")\n",
    "    \n",
    "    # Normalize after averaging\n",
    "    norm_magnitude = (avg_magnitude - avg_magnitude.min()) / (avg_magnitude.max() - avg_magnitude.min() + 1e-8)\n",
    "    \n",
    "    # Create RGB heatmap (red is high difference, blue is low)\n",
    "    avg_heatmap = torch.zeros(3, norm_magnitude.shape[0], norm_magnitude.shape[1])\n",
    "    avg_heatmap[0] = norm_magnitude  # Red channel\n",
    "    avg_heatmap[2] = 1 - norm_magnitude  # Blue channel\n",
    "    \n",
    "    # Save the average heatmap\n",
    "    if save_dir:\n",
    "        # Create full directory structure\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        ext = convert_to_ext if convert_to_ext else '.png'\n",
    "        save_path = os.path.join(save_dir, f\"average_heatmap{ext}\")\n",
    "        save_image(avg_heatmap, save_path)\n",
    "        print(f\"Average heatmap saved to {save_path}\")\n",
    "    \n",
    "    # Create and save visualization figure if requested\n",
    "    if figure_save_path:\n",
    "        # Create directory for figure\n",
    "        os.makedirs(os.path.dirname(figure_save_path), exist_ok=True)\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        \n",
    "        # Convert tensor to numpy for plotting\n",
    "        if avg_heatmap.shape[0] == 3:  # If RGB (from create_heatmap)\n",
    "            heatmap_np = avg_heatmap.permute(1, 2, 0).numpy()\n",
    "            im = ax.imshow(np.clip(heatmap_np, 0, 1))\n",
    "            \n",
    "            # Create custom blue-to-red colormap to match the heatmap\n",
    "            from matplotlib.colors import LinearSegmentedColormap\n",
    "            blue_to_red = LinearSegmentedColormap.from_list('BlueRed', [(0, 0, 1), (1, 0, 0)])\n",
    "            \n",
    "            # Create a separate colorbar with matching colors\n",
    "            import matplotlib as mpl\n",
    "            norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "            sm = plt.cm.ScalarMappable(cmap=blue_to_red, norm=norm)\n",
    "            sm.set_array([])\n",
    "            \n",
    "            cbar = fig.colorbar(sm, ax=ax, shrink=0.8, pad=0.02)\n",
    "        else:  # Fallback for grayscale\n",
    "            heatmap_np = avg_heatmap.squeeze(0).numpy()\n",
    "            im = ax.imshow(heatmap_np, cmap='coolwarm', vmin=0, vmax=1)\n",
    "            cbar = fig.colorbar(im, ax=ax, shrink=0.8, pad=0.02, linewidth=0)\n",
    "        \n",
    "        # Set up colorbar ticks and label\n",
    "        cbar.set_label('Normalized mean pixel-value difference', fontsize=28)\n",
    "        cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        cbar.ax.tick_params(labelsize=26)\n",
    "        \n",
    "        # Make colorbar border thicker\n",
    "        cbar.outline.set_linewidth(3)  # Adjust thickness as needed\n",
    "\n",
    "        # Make ticks thicker\n",
    "        cbar.ax.tick_params(width=3, length=10)  # Adjust width and length as needed\n",
    "        \n",
    "        # Remove the title\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Save figure with tight layout\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figure_save_path, bbox_inches='tight', dpi=600)\n",
    "        print(f\"Figure saved to {figure_save_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    return avg_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_heatmap = create_average_heatmap(\n",
    "    orig_dir='../imgproc_code/data/imagenet16_resized',\n",
    "    mod_dir='../imgproc_code/data/imagenet16_20_0.5_40_logit',\n",
    "    dirmap_path='../imgproc_code/data/imagenet16_resized/dirmap.csv',\n",
    "    figure_save_path='fig_outputs/avg_heatmap.pdf',\n",
    "    save_dir='heatmap_viz/imagenet16/average',\n",
    "    convert_to_ext='.png',\n",
    "    smooth_kernel_size=None,\n",
    "    splits=[\"val\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering images by difficulty (ground truth logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "def filter_and_save_by_logit(orig_dir, save_dir, logit_min, logit_max, splits=[\"val\"], max_images=None, max_per_class=None):\n",
    "    \"\"\"\n",
    "    Filter images based on robust_gt_logit values and save them with logit values in filenames,\n",
    "    maintaining roughly equal distribution across classes.\n",
    "    \n",
    "    Args:\n",
    "        orig_dir: Directory containing original images and dirmap.csv\n",
    "        save_dir: Directory to save filtered images\n",
    "        logit_min: Minimum robust_gt_logit value to include\n",
    "        logit_max: Maximum robust_gt_logit value to include\n",
    "        splits: List of dataset splits to include (default: [\"val\"])\n",
    "        max_images: Maximum total number of images to save (optional)\n",
    "        max_per_class: Maximum number of images per class (optional)\n",
    "            If neither max_images nor max_per_class is specified, all matching images will be saved\n",
    "            If both are specified, both limits will be applied\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing information about the saved images\n",
    "    \"\"\"\n",
    "    # Read directory mapping\n",
    "    dirmap = pd.read_csv(os.path.join(orig_dir, 'dirmap_logits.csv'))\n",
    "    \n",
    "    # Filter by split and logit range\n",
    "    mask = (dirmap[\"split\"].isin(splits)) & \\\n",
    "           (dirmap[\"robust_gt_logit\"] >= logit_min) & \\\n",
    "           (dirmap[\"robust_gt_logit\"] <= logit_max)\n",
    "    \n",
    "    filtered_df = dirmap[mask].copy()\n",
    "    \n",
    "    # Group by class\n",
    "    class_groups = filtered_df.groupby('class')\n",
    "    unique_classes = filtered_df['class'].unique()\n",
    "    n_classes = len(unique_classes)\n",
    "    \n",
    "    # Calculate images per class\n",
    "    if max_images is not None:\n",
    "        images_per_class = max(1, max_images // n_classes)\n",
    "        if max_per_class is not None:\n",
    "            images_per_class = min(images_per_class, max_per_class)\n",
    "    elif max_per_class is not None:\n",
    "        images_per_class = max_per_class\n",
    "    else:\n",
    "        # If no limits specified, use the size of the smallest class group\n",
    "        images_per_class = min(len(group) for _, group in class_groups)\n",
    "    \n",
    "    # Sample equal numbers from each class\n",
    "    balanced_dfs = []\n",
    "    for class_name, group in class_groups:\n",
    "        sampled = group.sample(n=min(len(group), images_per_class))\n",
    "        balanced_dfs.append(sampled)\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "    # If max_images is specified and we're still over the limit, sample randomly\n",
    "    if max_images is not None and len(balanced_df) > max_images:\n",
    "        balanced_df = balanced_df.sample(n=max_images)\n",
    "    \n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Process and save each image\n",
    "    saved_records = []\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    for _, row in balanced_df.iterrows():\n",
    "        # Get original image path\n",
    "        orig_path = os.path.join(orig_dir, row['im_path'])\n",
    "        \n",
    "        # Create new filename with logit value and class\n",
    "        base_name, ext = os.path.splitext(os.path.basename(row['im_path']))\n",
    "        logit_str = f\"{row['robust_gt_logit']:.3f}\".replace('-', 'neg')\n",
    "        new_filename = f\"{base_name}_class_{row['class']}_logit_{logit_str}{ext}\"\n",
    "        \n",
    "        # Create subdirectories if needed\n",
    "        rel_dir = os.path.dirname(row['im_path'])\n",
    "        save_subdir = os.path.join(save_dir, rel_dir)\n",
    "        os.makedirs(save_subdir, exist_ok=True)\n",
    "        \n",
    "        # Full save path\n",
    "        save_path = os.path.join(save_subdir, new_filename)\n",
    "        \n",
    "        # Copy image to new location\n",
    "        shutil.copy2(orig_path, save_path)\n",
    "        \n",
    "        # Update counters and records\n",
    "        class_counts[row['class']] += 1\n",
    "        saved_records.append({\n",
    "            'original_path': row['im_path'],\n",
    "            'saved_path': os.path.relpath(save_path, save_dir),\n",
    "            'robust_gt_logit': row['robust_gt_logit'],\n",
    "            'class': row['class']\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame of saved images\n",
    "    saved_df = pd.DataFrame(saved_records)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(save_dir, 'filtered_images_metadata.csv')\n",
    "    saved_df.to_csv(metadata_path, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nSaved {len(saved_df)} images total with robust_gt_logit values between {logit_min:.3f} and {logit_max:.3f}\")\n",
    "    print(\"\\nImages per class:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"Class {class_name}: {count} images\")\n",
    "    print(f\"\\nMetadata saved to: {metadata_path}\")\n",
    "    \n",
    "    return saved_df\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "filtered_df = filter_and_save_by_logit(\n",
    "    orig_dir='path/to/original/images',\n",
    "    save_dir='path/to/save/filtered/images',\n",
    "    logit_min=-1.0,\n",
    "    logit_max=1.0,\n",
    "    splits=[\"val\"],\n",
    "    max_images=100,  # optional: total image limit\n",
    "    max_per_class=20  # optional: per-class limit\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filter_and_save_by_logit(\n",
    "    orig_dir='../imgproc_code/data/imagenet16_resized',\n",
    "    save_dir='heatmap_viz/imagenet16_difficulty',\n",
    "    logit_min=20,\n",
    "    logit_max=30,\n",
    "    splits=[\"val\"],\n",
    "    max_images=200\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raevenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
