
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
    <meta name="description" content=""/>
    <meta name="author" content=""/>
    <title>L-WISE (Talbot, Kreiman, DiCarlo, Gaziv) </title>
    <!-- Favicon-->
    <link rel="icon" type="image/png" sizes="48x48" href="assets/icons/favicon-48x48.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="assets/icons/favicon-16x16.png">
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet"/>
    <link href="css/widget.css" rel="stylesheet"/>
</head>
<body>
<!-- Bootstrap core JS-->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>

<!-- Core libraries needed for the widget -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.4.1/papaparse.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.21/lodash.min.js"></script>


<!-- Page content-->
<div class="container" style="max-width:800px">
    <nav class="navbar bg-body-tertiary mt-lg-4">
        <h1>L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection And Enhancement</h1>

        <div class="container-fluid">
            <p class="lead">
                <a href="#" style="color:black; text-decoration:none">Morgan B. Talbot</a>,
                <a href="#" style="color:black; text-decoration:none">Gabriel Kreiman</a>,
                <a href="#" style="color:black; text-decoration:none">James J. DiCarlo</a>,
                and <a href="#" style="color:black; text-decoration:none">Guy Gaziv</a>
            </p>
        </div>

    </nav>

    <div class="container text-center">
        <div class="row">
            <div class="col">
                <p><a href="https://arxiv.org/pdf/2412.09765" class="link-primary link-offset-2 link-underline-opacity-25 link-underline-opacity-100-hover">Paper</a></p>
            </div>

            <div class="col">

                <p>

                        <a href="https://openreview.net/forum?id=AoIKgHu9Si" style="color:gray"
                          class="link-primary link-offset-2 link-underline-opacity-25 link-underline-opacity-100-hover">
                            OpenReview

                    </a>

                </p>

            </div>

            <div class="col">
                <p><a href="https://github.com/MorganBDT/L-WISE" class="link-primary link-offset-2 link-underline-opacity-25 link-underline-opacity-100-hover">Code</a></p>
            </div>
        </div>
    </div>
    <div class="text-left mt-4">
        <h3 style="color:gray">Abstract</h3>
        <p class="lead fs-6">
          The currently leading artificial neural network (ANN) models of the visual ventral stream — which are derived from a combination of performance optimization and robustification methods — have demonstrated a remarkable degree of behavioral alignment with humans on visual categorization tasks. 
          Extending upon previous work, we show that not only can these models guide image perturbations that change the induced human category percepts, but they also can enhance human ability to accurately report the original ground truth. 
          Furthermore, we find that the same models can also be used out-of-the-box to predict the proportion of correct human responses to individual images, providing a simple, human-aligned estimator of the relative difficulty of each image. 
          Motivated by these observations, we propose to augment visual learning in humans in a way that improves human categorization accuracy at test time. 
          Our learning augmentation approach consists of (i) selecting images based on their model-estimated recognition difficulty, and (ii) using image perturbations that aid recognition for novice learners. 
          We find that combining these model-based strategies gives rise to test-time categorization accuracy gains of 33-72% relative to control subjects without these interventions, despite using the same number of training feedback trials. 
          Surprisingly, beyond the accuracy gain, the training time for the augmented learning group was also shorter by 20-23%. 
          We demonstrate the efficacy of our approach in a fine-grained categorization task with natural images, as well as tasks in two clinically relevant image domains — histology and dermoscopy — where visual learning is notoriously challenging. 
          To the best of our knowledge, this is the first application of ANNs to increase visual learning performance in humans by enhancing category-specific features.
        </p>
    </div>


    <hr class="my-4"> <!-- Horizontal Line -->


    <!-- Predicting image difficulty paragraph -->
    <div class="text-left mt-lg-8 pt-4">

        <!-- Figure -->
        <figure class="figure">
          <div class="d-flex justify-content-center align-items-center">
              <img class="figure-img rounded img-fluid mx-auto pt-2 pb-4" src="assets/figures/teaser_simple.png" style="width:100%" alt="...">
          </div>
  
          <div class="d-flex justify-content-center align-items-center">
            <figcaption class="figure-caption text-left justify-content-center" style="max-width:92%">
              <b>ANN models can predict image difficulty for humans, and increase recognizability through “enhancement.”</b><br>
              The left-hand side shows the relationship between ANN “confidence” and the probability of a human choosing the correct ground truth category (“bird,” “lizard,” etc) after viewing an image for 17 milliseconds. 
              This shows that the model can predict which images are easy for humans to categorize and which are more difficult. 
              The right-hand side shows how human recognition accuracy increases as “enhancement” perturbations from an ANN become larger in magnitude - the more enhancement, the easier the images become for humans to categorize.  
            </figcaption>
          </div>
        </figure>


        <h2>Predicting image difficulty</h2>
        <p class="lead fs-6">
            We showed that robustified convolutional neural networks (CNNs) can make accurate predictions of image difficulty for humans. 
            Specifically, the pre-softmax logit activation score corresponding to the ground truth class for a given image is strongly correlated with the rate of correct recognition by humans who are shown the image for 17 milliseconds (see left-hand side of figure above). 
            This simple approach predicts image difficulty more accurately than previously developed metrics, including the equivalent logit scores from a non-robust model. 
            Accurate image difficulty predictions enable us to select images at appropriate levels of difficulty for novice learners of visual tasks. 
        </p>

    </div>


    <!-- Image enhancement paragraph -->
    <div class="text-left mt-lg-8 pt-4">

      <h2>Enhancing images to reduce difficulty</h2>
      <p class="lead fs-6">
        <a href="https://himjl.github.io/pwormholes/">Previous work</a> has shown that robustified CNNs can be used to generate small-magnitude image perturbations that strongly disrupt image recognition by humans.
        We apply a similar approach to <i>enhance</i> images, by optimizing the pixel values of an image to maximize the model-derived logit score of the ground truth class, the same value that we also use to predict image difficulty.
        We demonstrated for the first time that models can be used to augment human visual category perception, by effectively making images easier to recognize as a particular category (see right-hand side of figure above).
      </p>

  </div>

    <!-- ################# START OF WIDGET CODE ################# -->

    <!-- Add widget for image enhancement visualization -->
    <div class="text-left mt-4">
        <h2>Interactive visualization:</h2>
        <p class="lead fs-6 mb-3">
            Below, you can explore how our enhancement approach affects images from different datasets<sup><a style="text-decoration: none" href="#footnote:1">1</a></sup>. 
            Try adjusting the perturbation budget to see how the degree of enhancement affects the images.
            You can also see the difficulty score for each image predicted by a robust CNN (∈ [0,1], normalized by class). 
            The difficulty scores correspond to the original, unmodified images, not to the enhanced versions. 
        </p>

        <div class="container-fluid p-0">
            <!-- Dataset Selection -->
            <div class="row mb-0">
                <div class="col">
                    <div class="d-flex justify-content-center align-items-center">
                        <div class="button-container">
                            <div class="btn-group" role="group" aria-label="Dataset selection">
                                <input type="radio" class="btn-check" name="dataset_radio" id="dataset_imagenet" checked>
                                <label class="btn btn-outline-primary" for="dataset_imagenet" style="white-space: pre-line">ImageNet&#10;Animals</label>

                                <input type="radio" class="btn-check" name="dataset_radio" id="dataset_moths">
                                <label class="btn btn-outline-primary" for="dataset_moths" style="white-space: pre-line">iNaturalist&#10;Moths</label>

                                <input type="radio" class="btn-check" name="dataset_radio" id="dataset_dermoscopy">
                                <label class="btn btn-outline-primary" for="dataset_dermoscopy" style="white-space: pre-line">HAM10000&#10;Dermoscopy</label>

                                <input type="radio" class="btn-check" name="dataset_radio" id="dataset_histology">
                                <label class="btn btn-outline-primary" for="dataset_histology" style="white-space: pre-line">MHIST&#10;Histology</label>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Image Display Area -->
            <div class="row">
                <div class="col px-0">
                    <div class="image-carousel-container">
                        <!-- Left/Right Navigation -->
                        <button class="carousel-arrow left" id="carousel_prev">
                            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M15 18l-6-6 6-6"/>
                            </svg>
                        </button>
                        <button class="carousel-arrow right" id="carousel_next">
                            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M9 18l6-6-6-6"/>
                            </svg>
                        </button>

                        <!-- Images container -->
                        <div class="rows-container">
                            <div class="images-grid">
                                <!-- Original Images Row -->
                                <div class="row-container">
                                    <div class="image-row-label">Original&#10;images</div>
                                    <div class="image-row" id="original_images"></div>
                                </div>

                                <!-- Enhanced Images Row -->
                                <div class="row-container">
                                    <div class="image-row-label">Enhanced&#10;images</div>
                                    <div class="image-row" id="enhanced_images"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Epsilon Control -->
            <div class="row mt-0">
                <div class="col-md-8 offset-md-2">
                    <div class="epsilon-container">
                        <div class="epsilon-label">
                            Perturbation budget (ℓ₂ norm)
                        </div>
                        <input type="range" class="form-range" id="enhancement_epsilon_range" min="0" max="100" value="50">
                        <div class="epsilon-value">
                            (ε = <span id="current_epsilon">15</span>)
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- ################# END OF WIDGET CODE ################# -->

    <!-- Paragraph about boosting learning -->
    <div class="text-left mt-lg-8 pt-4">
      <h2>Boosting image category learning in humans</h2>

      <p class="lead fs-6">
        Humans learn most effectively when they are given an appropriate degree of challenge. 
        Some visual tasks, such as interpreting certain kinds of medical images, are simply too difficult for humans to efficiently learn by practicing with typical examples. 
        Using difficulty prediction and image enhancement as tools, we algorithmically designed curricula that start at a very easy level of difficulty and gradually increase the difficulty as learning progresses. 
        Specifically, we select model-identified "easy" images for the beginning of the learning process, and subsequently allow more and more difficult images to be selected. 
        We also enhance images (making them easier to recognize) with relatively large perturbations early on, and then gradually reduce the size of the perturbations.
        We showed that this strategy, which we call Logit-Weighted Image Selection and Enhancement (L-WISE), enables humans to learn faster and achieve higher scores when tested on held-out examples (unmodified, randomly selected images) across three different image categorization tasks (see figure below). 
        Two of the tasks are relevant to clinical medicine: distinguishing among several different skin lesion types in dermoscopy images, and distinguishing between benign and malignant tissue in colon histology images. 
      </p>

      <!-- Figure -->
      <figure class="figure"></figure>
        <div class="d-flex justify-content-center align-items-center">
            <img class="figure-img rounded img-fluid mx-auto pt-2 pb-4" src="assets/figures/learning_results_simple.png" style="width:90%" alt="...">
        </div>

        <div class="d-flex justify-content-center align-items-center">
          <figcaption class="figure-caption text-left justify-content-center" style="max-width:92%">
            <b>We applied difficulty prediction and image enhancement to create an "easy to hard" sequence of images, enabling humans to learn visual tasks more quickly and to higher levels of accuracy.</b><br>
            The left-hand panels illustrate our "L-WISE" learning assistance approach: we limit the inherent difficulty of examples presented early in the curriculum (top-left), and enhance images with larger-magnitude perturbations early in the curriculum before decreasing the perturbation magnitude as learning progresses (bottom-left).
            The right-hand panel shows how participants assisted by L-WISE completed the curriculum with shorter training times, and with higher final test accuracy (on unmodified, randomly-selected images), across three different visual tasks.  
          </figcaption>
        </div>
      </figure>

  </div>


    <hr class="my-4"> <!-- Horizontal Line -->
    <div class="container text-left">
        <h5>BibTeX (preprint)</h5>
        <p class="user-select-all"><code class="code-font">
          @inproceedings{talbot2025wise,<br>
            &nbsp;&nbsp;title={L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection And Enhancement},<br>
            &nbsp;&nbsp;author={Talbot, Morgan B and Kreiman, Gabriel and DiCarlo, James J and Gaziv, Guy},<br>
            &nbsp;&nbsp;booktitle={International Conference on Learning Representations},<br>
            &nbsp;&nbsp;year={2025}<br>
          }
        </code></p>
    </div>

    <hr class="my-4">
    <div class="container text-left">
        <h5>Footnotes</h5>
        <ol class="fs-6 lead">
            <li id="footnote:1"> Dataset sources: <br>
                <a href="https://www.image-net.org/">ImageNet (animals)</a>: Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.<br>
                <a href="https://www.kaggle.com/c/inaturalist-2021">iNaturalist (moths)</a>: Grant Van Horn and Oisin Mac Aodha. iNat Challenge 2021 - FGVC8. Kaggle, 2021.<br>
                <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T">HAM10000 (dermoscopy)</a>: Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The HAM10000 dataset, a large collection of multisource dermatoscopic images of common pigmented skin lesions. Scientific data, 5(1):1–9, 2018.<br>
                <a href="https://bmirds.github.io/MHIST/">MHIST (histology)</a>: Jerry Wei, Arief Suriawinata, Bing Ren, Xiaoying Liu, Mikhail Lisovsky, Louis Vaickus, Charles Brown, Michael Baker, Naofumi Tomita, Lorenzo Torresani, et al. A petri dish for histopathology image analysis. In Artificial Intelligence in Medicine: 19th International Conference on Artificial Intelligence in Medicine, AIME 2021, Virtual Event, June 15–18, 2021, Proceedings, pp. 11–24. Springer, 2021.
            </li>
        </ol>
    </div>

    <div class="text-left mt-4"></div>
        <h5>Acknowledgements</h5>
        <p class="lead fs-6">
            This work was supported in part by Harvard Medical School under the Dean’s Innovation Award for the
            Use of Artificial Intelligence, in part by Massachusetts Institute of Technology through the David and
            Beatrice Yamron Fellowship, in part by the National Institute of General Medical Sciences under Award
            T32GM144273, in part by the National Institutes of Health under Grant R01EY026025, and in part by
            the National Science Foundation under Grant CCF-1231216. The content is solely the responsibility of
            the authors and does not necessarily represent the official views of any of the above organizations. The
            authors would like to thank Andrei Barbu, Roy Ganz, Katherine Harvey, Michael J. Lee, Richard N.
            Mitchell, and Luke Rosedahl for sharing their helpful insights into our work at various times.
        </p>
    </div>
</div>


</div>

<!-- Core theme JS-->
<script type="module" src="js/widget.js"></script>

</body>
</html>
